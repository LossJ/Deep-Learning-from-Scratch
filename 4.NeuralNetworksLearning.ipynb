{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1、从数据中学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1、数据驱动"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器学习的方法则极力避免人为介入，尝试从收集到的数据中发现答案（模式）。  \n",
    "神经网络或深度学习则比以往的机器学习方法更能避免人为介入。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们来思考一个具体的问题，比如如何实现数字“5”的识别。\n",
    "<img src=\"./imgs/4_1.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果让我们自己来设计一个能将5正确分类的程序，就会意外地发现这是一个很难的问题。  \n",
    "人可以简单地识别出5，`但却很难明确说出是基于何种规律而识别出了5`。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与其绞尽脑汁，从零开始想出一个可以识别5的算法，不如考虑通过有效利用数据来解决这个问题。  \n",
    "一种方案是，先从图像中提取**特征量**,再用机器学习技术学习这些特征量的模式。  \n",
    "这里所说的“特征量”是指可以从输入数据（输入图像）中准确地提取本质数据（重要的数据）的转换器。图像的特征量通常表示为向量的形式。在计算机视觉领域，常用的特征量包括SIFT、 SURF和HOG等。使用这些特征量将图像数据转换为向量，然后对转换后的向量使用机器学习中的SVM、 KNN等分类器进行学习。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器学习的方法中，由机器从收集到的数据中找出规律性。  \n",
    "与从零开始想出算法相比，这种方法可以更高效地解决问题，也能减轻人的负担。  \n",
    "但是需要注意的是，将图像转换为向量时使用的特征量仍是由人设计的。  \n",
    "对于不同的问题，必须使用合适的特征量（必须设计专门的特征量），才能得到好的结果。  \n",
    "比如，为了区分狗的脸部，人们需要考虑与用于识别5的特征量不同的其他特征量。  \n",
    "也就是说，即使使用特征量和机器学习的方法，也需要针对不同的问题人工考虑合适的特征量。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到这里，我们介绍了两种针对机器学习任务的方法。将这两种方法用图来表示，如图4-2所示。  \n",
    "图中还展示了神经网络（深度学习）的方法，可以看出该方法不存在人为介入。\n",
    "<img src=\"./imgs/4_2.png\"></img>\n",
    "如图4-2所示，神经网络直接学习图像本身。  \n",
    "在第2个方法，即利用特征量和机器学习的方法中，特征量仍是由人工设计的，而在神经网络中，连图像中包含的重要特征量也都是由机器来学习的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度学习有时也称为端到端机器学习（end-to-end machine learning）。这里所说的**端到端**是指从一端到另一端的意思，也就是从原始数据（输入）中获得目标结果（输出）的意思。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`神经网络的优点是对所有的问题都可以用同样的流程来解决`。<br>\n",
    "比如，不管要求解的问题是识别5，还是识别狗，抑或是识别人脸，神经网络都是通过不断地学习所提供的数据，尝试发现待求解的问题的模式。  \n",
    "也就是说，与待处理的问题无关，神经网络可以将数据直接作为原始数据，进行“端对端”的学习。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2、训练数据和测试数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章主要介绍神经网络的学习，不过在这之前，我们先来介绍一下机器学习中有关数据处理的一些注意事项。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器学习中，一般将数据分为**训练数据**和**测试数据**两部分来进行学习和实验等。  \n",
    "首先，使用训练数据进行学习，寻找最优的参数；然后，使用测试数据评价训练得到的模型的实际能力。  \n",
    "为什么需要将数据分为训练数据和测试数据呢？因为我们追求的是模型的**泛化能力**。  \n",
    "为了正确评价模型的泛化能力，就必须划分训练数据和测试数据。另外，训练数据也可以称为**监督数据**。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**泛化能力**是指处理未被观察过的数据（不包含在训练数据中的数据）的能力。<br>\n",
    "`获得泛化能力是机器学习的最终目标`。<br>\n",
    "比如，在识别手写数字的问题中，泛化能力可能会被用在自动读取明信片的邮政编码的系统上。  \n",
    "此时，手写数字识别就必须具备较高的识别“某个人”写的字的能力。  \n",
    "注意这里不是“特定的某个人写的特定的文字”，而是“任意一个人写的任意文字”。  \n",
    "如果系统只能正确识别已有的训练数据，那有可能是只学习到了训练数据中的个人的习惯写法。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，`仅仅用一个数据集去学习和评价参数，是无法进行正确评价的`。<br>\n",
    "这样会导致可以顺利地处理某个数据集，但无法处理其他数据集的情况。  \n",
    "顺便说一下，只对某个数据集过度拟合的状态称为**过拟合**（over fitting）。  \n",
    "避免过拟合也是机器学习的一个重要课题。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2、损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络的学习通过某个指标表示现在的状态。然后，以这个指标为基准，寻找`最优权重参数`。  \n",
    "神经网络的学习中所用的指标称为**损失函数**（loss function）。  \n",
    "这个损失函数可以使用任意函数，但一般用`均方误差`和`交叉熵误差`等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数是表示神经网络性能的“恶劣程度”的指标，即当前的神经网络对监督数据在多大程度上不拟合，在多大程度上不一致。  \n",
    "以“性能的恶劣程度”为指标可能会使人感到不太自然，但是如果给损失函数乘上一个负值，就可以解释为“在多大程度上不坏”，即“性能有多好”。  \n",
    "并且，“使性能的恶劣程度达到最小”和“使性能的优良程度达到最大”是等价的，不管是用“恶劣程度”还是“优良程度”，做的事情本质上都是一样的。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1、均方误差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以用作损失函数的函数有很多，其中最有名的是均方误差（mean squared\n",
    "error）。均方误差如下式所示：$$\n",
    "E = \\frac { 1 } { 2 } \\sum _ { k } \\left( y _ { k } - t _ { k } \\right) ^ { 2 }\n",
    "$$<center>式（4.1）</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里， yk是表示神经网络的输出， tk表示监督数据， k表示数据的维数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比如，在3.6节手写数字识别的例子中， yk、 tk是由如下10个元素构成的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数组元素的索引从第一个开始依次对应数字“0”“1”“2”……   \n",
    "这里，神经网络的输出y是softmax函数的输出。由于softmax函数的输出可以理解为概率，因此上例表示“0”的概率是0.1，“1”的概率是0.05，“2”的概率是0.6\n",
    "等。   \n",
    "t是监督数据，将正确解标签设为1，其他均设为0。  \n",
    "这里，标签“2”为1，表示正确解是“2”。  \n",
    "将正确解标签表示为1，其他标签表示为0的表示方法称为**one-hot表示**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如式（4.1）所示，均方误差会计算神经网络的输出和正确解监督数据的各个元素之差的平方，再求总和。  现在，我们用Python来实现这个均方误差，  \n",
    "实现方式如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09750000000000003"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] #设“2”为正确解\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] #“2”的概率最高的情况（0.6）\n",
    "mean_squared_error(np.array(y),np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5975"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0] #“7”的概率最高的情况（0.6）\n",
    "mean_squared_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们发现第一个例子的损失函数的值更小，和监督数据之间的误差较小。  \n",
    "也就是说，均方误差显示第一个例子的输出结果与监督数据更加吻合。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2、交叉熵误差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了均方误差之外， **交叉熵误差**（cross entropy error）也经常被用作损失函数。  \n",
    "交叉熵误差如下式所示：$$\n",
    "E = - \\sum _ { k } t _ { k } \\log y _ { k }\n",
    "$$  \n",
    "<center>式（4.2）</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里， log表示以e为底数的自然对数（log e）。   \n",
    "yk是神经网络的输出， tk是正确解标签。并且， tk中只有正确解标签的索引为1，其他均为0（one-hot表示）。  \n",
    "因此，式（4.2）实际上只计算对应正确解标签的输出的自然对数。  \n",
    "比如，假设正确解标签的索引是“2”，与之对应的神经网络的输出是0.6，则交叉熵误差是−log 0.6 = 0.51；若“2”对应的输出是0.1，则交叉熵误差为−log 0.1 = 2.30。  \n",
    "也就是说，`交叉熵误差的值是由正确解标签所对应的输出结果决定的`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自然对数的图像如图4-3所示：\n",
    "<img src=\"./imgs/4_3.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x等于1时， y为0；随着x向0靠近， y逐渐变小。  \n",
    "因此，正确解标签对应的输出越大，式（4.2）的值越接近0；  \n",
    "当输出为1时，交叉熵误差为0。  \n",
    "此外，如果正确解标签对应的输出较小，则式（4.2）的值较大。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面，我们来用代码实现交叉熵误差:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数内部在计算np.log时，加上了一个微小值delta。  \n",
    "这是因为，当出现np.log(0)时， np.log(0)会变为负无限大的-inf，这样一来就会导致后续计算无法进行。  \n",
    "作为保护性对策，添加一个微小值可以防止负无限大的发生。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "cross_entropy_error(np.array(y),np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302584092994546"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个例子中，正确解标签对应的输出为0.6，此时的交叉熵误差大约为0.51。  \n",
    "第二个例子中，正确解标签对应的输出为0.1的低值，此时的交叉熵误差大约为2.3。  \n",
    "由此可以看出，这些结果与我们前面讨论的内容是一致的。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3、mini-batch学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器学习使用训练数据进行学习。  \n",
    "使用训练数据进行学习，严格来说，就是针对训练数据计算损失函数的值，找出使该值尽可能小的参数。  \n",
    "因此，计算损失函数时必须将所有的训练数据作为对象。  \n",
    "也就是说，如果训练数据有100个的话，我们就要把这100个损失函数的总和作为学习的指标。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果要求所有训练数据的损失函数的总和，以交叉熵误差为例，可以写成下面\n",
    "的式（4.3）：$$\n",
    "E = - \\frac { 1 } { N } \\sum _ { n } \\sum _ { k } t _ { n k } \\log y _ { n k }\n",
    "$$  \n",
    "<center>式（4.3）</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里,假设数据有N个， tnk表示第n个数据的第k个元素的值（ynk是神经网络的输出， tnk是监督数据）。  \n",
    "式子虽然看起来有一些复杂，其实只是把`求单个数据的损失函数的式（4.2）扩大到了N份数据，不过最后还要除以N进行正规化`。  \n",
    "通过除以N，可以求单个数据的“平均损失函数”。  \n",
    "通过这样的平均化，可以获得和训练数据的数量无关的统一指标。  \n",
    "比如，即便训练数据有1000个或10000个，也可以求得单个数据的平均损失函数。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外， MNIST数据集的训练数据有60000个，如果以全部数据为对象求损失函数的和，则计算过程需要花费较长的时间。  \n",
    "再者，如果遇到大数据，数据量会有几百万、几千万之多，这种情况下以全部数据为对象计算损失函\n",
    "数是不现实的。  \n",
    "因此，我们从全部数据中选出一部分，作为全部数据的“近似”。  \n",
    "神经网络的学习也是从训练数据中选出一批数据（称为mini-batch,小批量），然后对每个mini-batch进行学习。  \n",
    "比如，从60000个训练数据中随机选择100笔，再用这100笔数据进行学习。  \n",
    "这种学习方式称为**mini-batch学习**。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们来编写从训练数据中随机选择指定个数的数据的代码，以进行mini-batch学习。  \n",
    "在这之前，先来看一下用于读入MNIST数据集的代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "print(x_train.shape) # (60000, 784)\n",
    "print(t_train.shape) # (60000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load_mnist函数是用于读入MNIST数据集的函数。这个\n",
    "函数在本书提供的脚本dataset/mnist.py中，它会读入训练数据和测试数据。  \n",
    "参 数normalize设置是否将输入图像正规化为0.0～1.0的值。  \n",
    "如果将该参数设置为 False，则输入图像的像素会保持原来的0～255。  \n",
    "设定参数 one_hot_label=True，可以得到one-hot表示（即仅正确解标签为1，其余为0的数据结构）。    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读入上面的MNIST数据后，训练数据有60000个，输入数据是784维（28 × 28）的图像数据，监督数据是10维的数据。  \n",
    "因此，上面的 x_train、 t_train的形状分别是(60000, 784)和(60000, 10)。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么，如何从这个训练数据中随机抽取10笔数据呢？  \n",
    "我们可以使用NumPy的np.random.choice()，写成如下形式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch\n",
    "t_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用np.random.choice()可以从指定的数字中随机选择想要的数字。  \n",
    "比如，np.random.choice(60000, 10)会从0到59999之间随机选择10个数字。  \n",
    "如下面的实际代码所示，我们可以得到一个包含被选数据的索引的数组：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([40619, 18848,  7065, 58149, 24765, 41364,  3277,  5178, 59158,\n",
       "       29328])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(60000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之后，我们只需指定这些随机选出的索引，取出mini-batch，然后使用这个mini-batch计算损失函数即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算电视收视率时，并不会统计所有家庭的电视机，而是仅以那些被选中的家庭为统计对象。  \n",
    "比如，通过从关东地区随机选择1000个家庭计算收视率，可以近似地求得关东地区整体的收视率。  \n",
    "这1000个家庭的收视率，虽然严格上不等于整体的收视率，但可以作为整体的一个近似值。  \n",
    "和收视率一样，mini-batch的损失函数也是利用一部分样本数据来近似地计算整体。  \n",
    "也就是说，用随机选择的小批量数据（mini-batch）作为全体训练数据的近似值。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4、mini-batch版交叉熵误差的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何实现对应mini-batch的交叉熵误差呢？  \n",
    "只要改良一下之前实现的对应单个数据的交叉熵误差就可以了。  \n",
    "这里，我们来实现一个可以同时处理单个数据和批量数据（数据作为batch集中输入）两种情况的函数。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里， y是神经网络的输出， t是监督数据。   \n",
    "y的维度为1时，即求单个数据的交叉熵误差时，需要改变数据的形状。  \n",
    "并且，当输入为mini-batch时，要用batch的个数进行正规化，计算单个数据的平均交叉熵误差。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当监督数据是标签形式（非one-hot表示，而是像“2”“7”这样的标签）时，交叉熵误差可通过如下代码实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现的要点是，由于one-hot表示中t为0的元素的交叉熵误差也为0，因此针对这些元素的计算可以忽略。  \n",
    "换言之，如果可以获得神经网络在正确解标签处的输出，就可以计算交叉熵误差。  \n",
    "因此， t为 one-hot表示时通过t * np.log(y)计算的地方，在 t为标签形式时，可用 np.log( y[np.arange(batch_size), t] )实现相同的处理（为了便于观察，这里省略了微小值1e-7）。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作为参考，简单介绍一下np.log( y[np.arange(batch_size), t] )。   \n",
    "np.arange(batch_size)会生成一个从0到 batch_size-1的数组。  \n",
    "比如当 batch_size为5时， np.arange(batch_size)会生成一个NumPy 数组 [0, 1, 2, 3, 4]。  \n",
    "因为t中标签是以 [2, 7, 0, 9, 4]的形式存储的，所以 y[np.arange(batch_size),\n",
    "t]能抽出各个数据的正确解标签对应的神经网络的输出（在这个例子中，y[np.arange(batch_size), t] 会 生 成 NumPy 数 组[y[0,2],y[1,7],y[2,0],y[3,9], y[4,4]]）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.5、为何要设定损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以数字识别任务为例，我们想获得的是能提高识别精度的参数，特意再导入一个损失函数不是有些重复劳动吗？  \n",
    "也就是说，既然我们的目标是获得使识别精度尽可能高的神经网络，那不是应该把识别精度作为指标吗？  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于这一疑问，我们可以根据“导数”在神经网络学习中的作用来回答。  \n",
    "下一节中会详细说到，在神经网络的学习中，寻找最优参数（权重和偏置）时，要寻找使损失函数的值尽可能小的参数。  \n",
    "为了找到使损失函数的值尽可能小的地方，需要计算参数的导数（确切地讲是梯度），然后以这个导数为指引，逐步更新参数的值。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设有一个神经网络，现在我们来关注这个神经网络中的某一个权重参数。  \n",
    "此时，对该权重参数的损失函数求导，表示的是“如果稍微改变这个权重参数的值，损失函数的值会如何变化”。  \n",
    "如果导数的值为负，通过使该权重参数向正方向改变，可以减小损失函数的值；反过来，如果导数的值为正，则通过使该权重参数向负方向改变，可以减小损失函数的值。  \n",
    "不过，当导数的值为0时，无论权重参数向哪个方向变化，损失函数的值都不会改变，此时该权重参数的更新会停在此处。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之所以不能用识别精度作为指标，是因为这样一来绝大多数地方的导数\n",
    "都会变为0，导致参数无法更新。  \n",
    "我们来总结一下上面的内容："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "在进行神经网络的学习时，不能将识别精度作为指标。\n",
    "因为如果以识别精度为指标，则参数的导数在绝大多数地方都会变为0。\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么用识别精度作为指标时，参数的导数在绝大多数地方都会变成0呢？  \n",
    "为了回答这个问题，我们来思考另一个具体例子。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设某个神经网络正确识别出了100笔训练数据中的32笔，此时识别精度为32 %。  \n",
    "如果以识别精度为指标，即使稍微改变权重参数的值，识别精度也仍将保持在32 %，不会\n",
    "出现变化。  \n",
    "也就是说，仅仅微调参数，是无法改善识别精度的。  \n",
    "即便识别精度有所改善，它的值也不会像32.0123 . . . %这样连续变化，而是变为33 %、\n",
    "34 %这样的不连续的、离散的值。  \n",
    "而如果把损失函数作为指标，则当前损失函数的值可以表示为0.92543 . . . 这样的值。  \n",
    "并且，如果稍微改变一下参数的值，对应的损失函数也会像0.93432 . . . 这样发生连续性的变化。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "识别精度对微小的参数变化基本上没有什么反应，即便有反应，它的值也是不连续地、突然地变化。  \n",
    "作为激活函数的阶跃函数也有同样的情况。  \n",
    "出于相同的原因，如果使用阶跃函数作为激活函数，神经网络的学习将无法进行。  \n",
    "如图4-4所示，阶跃函数的导数在绝大多数地方（除了0以外的地方）均为0。  \n",
    "也就是说，如果使用了阶跃函数，那么即便将损失函数作为指标，参数的微小变化也会被阶跃函数抹杀，导致损失函数的值不会产生任何变化。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "阶跃函数就像“竹筒敲石”一样，只在某个瞬间产生变化。  \n",
    "而sigmoid函数，如图4-4所示，不仅函数的输出（竖轴的值）是连续变化的，曲线的斜率（导数）也是连续变化的。  \n",
    "也就是说， `sigmoid函数的导数在任何地方都不为0`，这对神经网络的学习非常重要。  \n",
    "得益于这个斜率不会为0的性质，神经网络的学习得以正确进行。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/4_4.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3、数值微分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`梯度法使用梯度的信息决定前进的方向`。<br>\n",
    "本节将介绍梯度是什么、有什么性质等内容。在这之前，我们先来介绍一下导数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1、导数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假如你是全程马拉松选手，在开始的10分钟内跑了2千米。  \n",
    "如果要计算此时的奔跑速度，则为2/10 = 0.2［千米/分］。  \n",
    "也就是说，你以1分钟前进0.2千米的速度（变化）奔跑。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个马拉松的例子中，我们计算了“奔跑的距离”相对于“时间”发生了多大变化。  \n",
    "不过，这个10分钟跑2千米的计算方式，严格地讲，计算的是10分钟内的平均速度。  \n",
    "而导数表示的是某个瞬间的变化量。  \n",
    "因此，将10分钟这一时间段尽可能地缩短，比如计算前1分钟奔跑的距离、前1秒钟奔跑的距离、前0.1秒钟奔跑的距离……这样就可以获得某个瞬间的变化量（某个瞬时速度）。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "综上，导数就是表示某个瞬间的变化量。它可以定义成下面的式子：\n",
    "$$\n",
    "\\frac { \\mathrm { d } f ( x ) } { \\mathrm { d } x } = \\lim _ { h \\rightarrow 0 } \\frac { f ( x + h ) - f ( x ) } { h }\n",
    "$$\n",
    "<center>式（4.4）</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "式（4.4）表示的是函数的导数。  \n",
    "左边的符号表示f（x）关于x的导数，即f（x）相对于x的变化程度。  \n",
    "式（4.4）表示的导数的含义是， x的“微小变化”将导致函数f（x）的值在多大程度上发生变化。  \n",
    "其中，表示微小变化的h无限趋近0，表示为：$$\n",
    "\\lim _ { h \\rightarrow 0 }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们参考式（4.4），来实现求函数的导数的程序。  \n",
    "如果直接实现式（4.4）的话，向h中赋入一个微小值，就可以计算出来了。  \n",
    "比如，下面的实现如何？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不好的实现示例\n",
    "def numerical_diff(f, x):\n",
    "    h = 10e-50\n",
    "    return (f(x+h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数numerical_diff(f, x)的名称来源于**数值微分**的英文numerical differentiation。  \n",
    "这个函数有两个参数，即“函数f”和“传给函数f的参数x”。  \n",
    "乍一看这个实现没有问题，但是实际上这段代码有两处需要改进的地方。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的实现中，因为想把尽可能小的值赋给 h（可以话，想让h无限接近0），所以h使用了10e-50（有50个连续的0的“0.00 . . . 1”）这个微小值。  \n",
    "但是，这样反而产生了**舍入误差**（rounding error）。  \n",
    "所谓舍入误差，是指因省略小数的精细部分的数值（比如，小数点第8位以后的数值）而造成最终的计算结果上的误差。  \n",
    "比如，在Python中，舍入误差可如下表示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.float32(1e-50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果用float32类型（32位的浮点数）来表示1e-50，就会变成0.0，无法正确表示出来。  \n",
    "也就是说，使用过小的值会造成计算机出现计算上的问题。  \n",
    "这是第一个需要改进的地方，即将微小值 h改为10−4。  \n",
    "使用10−4就可以得到正确的结果。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二个需要改进的地方与函数f的差分有关。  \n",
    "虽然上述实现中计算了函数 f在 x+h和 x之间的差分，但是必须注意到，这个计算从一开始就有误差。\n",
    "如图4-5所示，“真的导数”对应函数在x处的斜率（称为切线），但上述实现中计算的导数对应的是(x + h)和x之间的斜率。  \n",
    "因此，真的导数（真的切线）和上述实现中得到的导数的值在严格意义上并不一致。  \n",
    "这个差异的出现是因为h不可能无限接近0。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/4_5.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如图4-5所示，数值微分含有误差。  \n",
    "为了减小这个误差，我们可以计算函数f在(x + h)和(x − h)之间的差分。  \n",
    "因为这种计算方法以x为中心，计算它左右两边的差分，所以也称为**中心差分**（而(x + h)和x之间的差分称为**前向差分**）。  \n",
    "下面，我们基于上述两个要改进的点来实现数值微分（数值梯度）。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    return (f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上所示，利用微小的差分求导数的过程称为数值微分（numerical differentiation）。  \n",
    "而基于数学式的推导求导数的过程，则用“解析性”（analytic）一词，称为“解析性求解”或者“解析性求导”。  \n",
    "比如，$$\n",
    "y = x ^ { 2 }\n",
    "$$的导数，可以通过$$\n",
    "\\frac { \\mathrm { d } y } { \\mathrm { d } x } = 2 x\n",
    "$$解析性地求解出来。  \n",
    "因此，当x = 2时，y的导数为4。  \n",
    "解析性求导得到的导数是不含误差的“真的导数”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2、数值微分的例子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们试着用上述的数值微分对简单函数进行求导。先来看一个由下\n",
    "式表示的2次函数：$$\n",
    "y = 0.01 x ^ { 2 } + 0.1 x\n",
    "$$\n",
    "<center>式（4.5）</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'x')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'f(x)')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11220a518>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "x = np.arange(0.0, 20.0, 0.1) # 以0.1为单位，从0到20的数组x\n",
    "y = function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x, y)\n",
    "plt.show()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来计算一下这个函数在x = 5和x = 10处的导数:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1999999999990898"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.2999999999986347"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function_1, 5)\n",
    "numerical_diff(function_1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里计算的导数是f(x)相对于x的变化量，对应函数的斜率。  \n",
    "另外，$$\n",
    "f ( x ) = 0.01 x ^ { 2 } + 0.1 x\n",
    "$$的 解析解是$$\n",
    "\\frac { \\mathrm { d } f ( x ) } { \\mathrm { d } x } = 0.02 x + 0.1\n",
    "$$。\n",
    "因 此，在 x = 5 和x = 10处，“真的导数”分别为0.2和0.3。  \n",
    "和上面的结果相比，我们发现虽然严格意义上它们并不一致，但误差非常小。  \n",
    "实际上，误差小到基本上可以认为它们是相等的。  \n",
    "现在，我们用上面的数值微分的值作为斜率，画一条直线。结果如图4-7所示，可以确认这些直线确实对应函数的切线。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/4_7.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3、偏导数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们看一下式(4.6)表示的函数。  \n",
    "虽然它只是一个计算参数的平方和的简单函数，但是请注意和上例不同的是，这里有两个变量。  \n",
    "$$\n",
    "f \\left( x _ { 0 } , x _ { 1 } \\right) = x _ { 0 } ^ { 2 } + x _ { 1 } ^ { 2 }\n",
    "$$\n",
    "<center>式（4.6）</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "    # 或者return np.sum(x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，我们假定向参数输入了一个NumPy数组。  \n",
    "函数的内部实现比较简单，先计算NumPy数组中各个元素的平方，再求它们的和（np.sum(x**2)也可以实现同样的处理）。  \n",
    "我们来画一下这个函数的图像。结果如图4-8所示，是一个三维图像。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/4_8.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们来求式（4.6）的导数。  \n",
    "这里需要注意的是，式（4.6）有两个变量，所以有必要区分对哪个变量求导数，即对x0和x1两个变量中的哪一个求导数。  \n",
    "另外，我们把这里讨论的有多个变量的函数的导数称为偏导数。  \n",
    "用数学式表示的话，可以写成:$$\n",
    "\\frac { \\partial f } { \\partial x _ { 0 } } 、 \\frac { \\partial f } { \\partial x _ { 1 } }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "怎么求偏导数呢？我们先试着解一下下面两个关于偏导数的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问题1：求x0 = 3, x1 = 4时，关于x0的偏导数$$\n",
    "\\frac { \\partial f } { \\partial x _ { 0 } }\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_tmp1(x0):\n",
    "    return x0*x0 + 4.0**2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function_tmp1, 3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问题2：求x0 = 3, x1 = 4时，关于x1的偏导数$$\n",
    "\\frac { \\partial f } { \\partial x _ { 1 } }\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_tmp2(x1):\n",
    "    return 3.0**2.0 + x1*x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function_tmp2, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这些问题中，我们定义了一个只有一个变量的函数，并对这个函数进行了求导。  \n",
    "例如，问题1中，我们定义了一个固定x1 = 4的新函数，然后对只有变量x0的函数应用了求数值微分的函数。  \n",
    "从上面的计算结果可知，问题1的答案是6.00000000000378，问题2的答案7.999999999999119，和解析解的导数基本一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "像这样，偏导数和单变量的导数一样，都是求某个地方的斜率。  \n",
    "不过，偏导数需要将多个变量中的某一个变量定为目标变量，并将其他变量固定为某个值。  \n",
    "在上例的代码中，为了将目标变量以外的变量固定到某些特定的值上，我们定义了新函数。  \n",
    "然后，对新定义的函数应用了之前的求数值微分的函数，得到偏导数。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4、梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在刚才的例子中，我们按变量分别计算了x0和x1的偏导数。  \n",
    "现在，我们希望一起计算x0和x1的偏导数$$\n",
    "\\left( \\frac { \\partial f } { \\partial x _ { 0 } } , \\frac { \\partial f } { \\partial x _ { 1 } } \\right)\n",
    "$$  \n",
    "比如，我们来考虑求x0 = 3, x1 = 4时(x0, x1)的偏导数 。  \n",
    "另外，像这样的由全部变量的偏导数汇总而成的向量称为**梯度**（gradient）。  \n",
    "梯度可以像下面这样来实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x) # 生成和x形状相同的数组\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        # f(x+h)的计算\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        # f(x-h)的计算\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val # 还原值\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数 numerical_gradient(f, x)的实现看上去有些复杂，但它执行的处理和求单变量的数值微分基本没有区别。  \n",
    "需要补充说明一下的是， np.zeros_like(x)会生成一个形状和x相同、所有元素都为0的数组。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数numerical_gradient(f, x)中，参数f为函数，x为NumPy数组，该函数对NumPy数组x的各个元素求数值微分。  \n",
    "现在，我们用这个函数实际计算一下梯度。  \n",
    "这里我们求点(3, 4)、 (0, 2)、 (3, 0)处的梯度。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 8.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0., 4.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([6., 0.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([3.0, 4.0]))\n",
    "numerical_gradient(function_2, np.array([0.0, 2.0]))\n",
    "numerical_gradient(function_2, np.array([3.0, 0.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "像这样，我们可以计算(x0, x1)在各点处的梯度。  \n",
    "上例中，点(3, 4)处的梯度是(6, 8)、点(0, 2)处的梯度是(0, 4)、点(3, 0)处的梯度是(6, 0)。  \n",
    "这个梯度意味着什么呢？  \n",
    "为了更好地理解，我们把$$\n",
    "f \\left( x _ { 0 } + x _ { 1 } \\right) = x _ { 0 } ^ { 2 } + x _ { 1 } ^ { 2 }\n",
    "$$的梯度画在图上。  \n",
    "不过，这里我们画的是元素值为负梯度 B 的向量。  \n",
    "<img src=\"./imgs/4_9.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如图4-9所示， $$\n",
    "f \\left( x _ { 0 } + x _ { 1 } \\right) = x _ { 0 } ^ { 2 } + x _ { 1 } ^ { 2 }\n",
    "$$的梯度呈现为有向向量（箭头）。  \n",
    "观察图4-9，我们发现梯度指向函数f(x0,x1)的“最低处”（最小值），就像指南针\n",
    "一样，所有的箭头都指向同一点。  \n",
    "其次，我们发现离“最低处”越远，箭头越大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然图 4-9 中的梯度指向了最低处，但并非任何时候都这样。  \n",
    "实际上，梯度会指向各点处的函数值降低的方向。  \n",
    "更严格地讲，梯度指示的方向是各点处的函数值减小最多的方向。  \n",
    "这是一个非常重要的性质，请一定牢记!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1　梯度法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器学习的主要任务是在学习时寻找最优参数。  \n",
    "同样地，神经网络也必须在学习时找到最优参数（权重和偏置）。  \n",
    "这里所说的最优参数是指损失函数取最小值时的参数。  \n",
    "但是，一般而言，损失函数很复杂，参数空间庞大，我们不知道它在何处能取得最小值。  \n",
    "而通过巧妙地使用梯度来寻找函数最小值（或者尽可能小的值）的方法就是梯度法。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里需要注意的是，梯度表示的是各点处的函数值减小最多的方向。  \n",
    "因此，无法保证梯度所指的方向就是函数的最小值或者真正应该前进的方向。  \n",
    "实际上，在复杂的函数中，梯度指示的方向基本上都不是函数值最小处。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数的极小值、最小值以及被称为**鞍点**（saddle point） 的地方，梯度为0。  \n",
    "极小值是局部最小值，也就是限定在某个范围内的最小值。  \n",
    "鞍点是从某个方向上看是极大值，从另一个方向上看则是极小值的点。  \n",
    "虽然梯度法是要寻找梯度为0的地方，但是那个地方不一定就是最小值（也有可能是极小值或者鞍点）。  \n",
    "此外，当函数很复杂且呈扁平状时，学习可能会进入一个（几乎）平坦的地区，陷入被称为“学习高原”的无法前进的停滞期。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然梯度的方向并不一定指向最小值，但沿着它的方向能够最大限度地减小函数的值。  \n",
    "因此，在寻找函数的最小值（或者尽可能小的值）的位置的任务中，要以梯度的信息为线索，决定前进的方向。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时梯度法就派上用场了。  \n",
    "在梯度法中，函数的取值从当前位置沿着梯度方向前进一定距离，然后在新的地方重新求梯度，再沿着新梯度方向前进，如此反复，不断地沿梯度方向前进。  \n",
    "像这样，通过不断地沿梯度方向前进，逐渐减小函数值的过程就是**梯度法**（gradient method）。  \n",
    "梯度法是解决机器学习中最优化问题的常用方法，特别是在神经网络的学习中经常被使用。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据目的是寻找最小值还是最大值，梯度法的叫法有所不同。  \n",
    "严格地讲，寻找最小值的梯度法称为**梯度下降法**（gradient descent method），寻找最大值的梯度法称为**梯度上升法**（gradient ascent method）。  \n",
    "但是通过反转损失函数的符号，求最小值的问题和求最大值的问题会变成相同的问题，因此“下降”还是“上升”的差异本质上并不重要。  \n",
    "一般来说，神经网络（深度学习）中，梯度法主要是指梯度下降法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们尝试用数学式来表示梯度法，如式（4.7）所示。\n",
    "$$\n",
    "\\begin{array} { l } { x _ { 0 } = x _ { 0 } - \\eta \\frac { \\partial f } { \\partial x _ { 0 } } } \\\\ { x _ { 1 } = x _ { 1 } - \\eta \\frac { \\partial f } { \\partial x _ { 1 } } } \\end{array}\n",
    "$$\n",
    "<center>式（4.7）</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "式（4.7）的η表示更新量，在神经网络的学习中，称为**学习率**（learning rate）。  \n",
    "学习率决定在一次学习中，应该学习多少，以及在多大程度上更新参数。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "式（4.7）是表示更新一次的式子，这个步骤会反复执行。  \n",
    "也就是说，每一步都按式（4.7）更新变量的值，通过反复执行此步骤，逐渐减小函数值。  \n",
    "虽然这里只展示了有两个变量时的更新过程，但是即便增加变量的数量，也可以通过类似的式子（各个变量的偏导数）进行更新。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学习率需要事先确定为某个值，比如0.01或0.001。  \n",
    "一般而言，这个值过大或过小，都无法抵达一个“好的位置”。  \n",
    "在神经网络的学习中，一般会一边改变学习率的值，一边确认学习是否正确进行了。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用Python来实现梯度下降法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数 f是要进行最优化的函数， init_x是初始值， lr是学习率learning\n",
    "rate，step_num是梯度法的重复次数。   \n",
    "numerical_gradient(f,x)会求函数的梯度，用该梯度乘以学习率得到的值进行更新操作，由step_num指定重复的次数。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用这个函数可以求函数的极小值，顺利的话，还可以求函数的最小值。\n",
    "下面，我们就来尝试解决下面这个问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问题：请用梯度法求$$\n",
    "f \\left( x _ { 0 } + x _ { 1 } \\right) = x _ { 0 } ^ { 2 } + x _ { 1 } ^ { 2 }\n",
    "$$的最小值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，设初始值为 (-3.0, 4.0)，开始使用梯度法寻找最小值。  \n",
    "最终的结果是 (-6.1e-10, 8.1e-10)，非常接近 (0， 0)。  \n",
    "实际上，真的最小值就是 (0， 0)，所以说通过梯度法我们基本得到了正确结果。  \n",
    "如果用图来表示梯度法的更新过程，则如图4-10所示。  \n",
    "可以发现，原点处是最低的地方，函数的取值一点点在向其靠近。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/4_10.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面说过，学习率过大或者过小都无法得到好的结果。我们来做个实验\n",
    "验证一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.58983747e+13, -1.29524862e+12])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#学习率过大的例子： lr=10.0\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.99999994,  3.99999992])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#学习率过小的例子： lr=1e-10\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=1e-10, step_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验结果表明，学习率过大的话，会发散成一个很大的值；  \n",
    "反过来，学习率过小的话，基本上没怎么更新就结束了。  \n",
    "也就是说，设定合适的学习率是一个很重要的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "像学习率这样的参数称为**超参数**。  \n",
    "这是一种和神经网络的参数（权重和偏置）性质不同的参数。  \n",
    "相对于神经网络的权重参数是通过训练数据和学习算法自动获得的，学习率这样的超参数则是人工设定的。  \n",
    "一般来说，超参数需要尝试多个值，以便找到一种可以使学习顺利进行的设定。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2、神经网络的梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络的学习也要求梯度。  \n",
    "这里所说的梯度是指损失函数关于权重参数的梯度。  \n",
    "比如，有一个只有一个形状为2 × 3的权重W的神经网络，损失函数用L表示。  \n",
    "此时，梯度可以用$$\n",
    "\\frac { \\partial L } { \\partial W }\n",
    "$$表示。用数学式表示的话，如下所示：$$\n",
    "\\begin{aligned} \\boldsymbol { W } & = \\left( \\begin{array} { c c c } { w _ { 11 } } & { w _ { 12 } } & { w _ { 13 } } \\\\ { w _ { 21 } } & { w _ { 22 } } & { w _ { 23 } } \\end{array} \\right) \\\\ \\frac { \\partial L } { \\partial \\boldsymbol { W } } & = \\left( \\begin{array} { c c c } { \\frac { \\partial L } { \\partial w _ { 11 } } } & { \\frac { \\partial L } { \\partial w _ { 12 } } } & { \\frac { \\partial L } { \\partial w _ { 13 } } } \\\\ { \\frac { \\partial L } { \\partial w _ { 21 } } } & { \\frac { \\partial L } { \\partial w _ { 22 } } } & { \\frac { \\partial L } { \\partial w _ { 23 } } } \\end{array} \\right) \\end{aligned}\n",
    "$$\n",
    "<center>式（4.8）</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac { \\partial L } { \\partial W }\n",
    "$$的元素由各个元素关于W的偏导数构成。  \n",
    "比如，第1行第1列的元素$$\n",
    "\\frac { \\partial L } { \\partial w _ { 11 } }\n",
    "$$表示当w11稍微变化时，损失函数L会发生多大变化。  \n",
    "这里的重点是，$$\n",
    "\\frac { \\partial L } { \\partial W }\n",
    "$$的形状和W相同。  \n",
    "实际上，式（4.8）中的W和$$\n",
    "\\frac { \\partial L } { \\partial W }\n",
    "$$都是2 × 3的形状。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面，我们以一个简单的神经网络为例，来实现求梯度的代码。  \n",
    "为此，我们要实现一个名为 simpleNet的类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # 用高斯分布进行初始化\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里使用了 common/functions.py中的 softmax和 cross_entropy_error方法，以及common/gradient.py中的numerical_gradient方法。  \n",
    "simpleNet类只有一个实例变量，即形状为2× 3的权重参数。  \n",
    "它有两个方法，一个是用于预测的 predict(x)，另一个是用于求损失函数值的 loss(x,t)。  \n",
    "这里参数 x接收输入数据， t接收正确解标签。  \n",
    "现在我们来试着用一下这个simpleNet。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.39534965, 1.79930307, 0.78469844],\n",
       "       [0.4861724 , 0.17423563, 0.10376392]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "net.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.67476495, 1.2363939 , 0.56420659])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p) # 最大值的索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4049729266716486"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0, 0, 1]) # 正确解标签\n",
    "net.loss(x,t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来求梯度。和前面一样，我们使用 numerical_gradient(f, x)求梯度（这里定义的函数f(W)的参数W是一个伪参数。  \n",
    "因为numerical_gradient(f,x)会在内部执行f(x),为了与之兼容而定义了f(W)）。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(W):\n",
    "    return net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.16443482,  0.28834083, -0.45277566],\n",
       "       [ 0.24665223,  0.43251125, -0.67916348]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW = numerical_gradient(f, net.W)\n",
    "dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numerical_gradient(f, x) 的参数f是函数， x是传给函数f的参数。  \n",
    "因此，这里参数x取 net.W，并定义一个计算损失函数的新函数f，然后把这个新定义的函数传递给numerical_gradient(f, x)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numerical_gradient(f, net.W)的结果是dW，一个形状为2 × 3的二维数组。\n",
    "观察一下dW的内容，例如，会发现$$\n",
    "\\frac { \\partial L } { \\partial W }\n",
    "$$中的$$\n",
    "\\frac { \\partial L } { \\partial w _ { 11 } }\n",
    "$$的值大约是0.2，这表示如\n",
    "果将w11增加h，那么损失函数的值会增加0.2h。再如， $$\n",
    "\\frac { \\partial L } { \\partial w _ { 23 } }\n",
    "$$对应的值大约\n",
    "是−0.5，这表示如果将w23增加h，损失函数的值将减小0.5h。因此，从减\n",
    "小损失函数值的观点来看， w23应向正方向更新， w11应向负方向更新。至于\n",
    "更新的程度， w23比w11的贡献要大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外，在上面的代码中，定义新函数时使用了“def f(x):···”的形式。  \n",
    "实际上， Python中如果定义的是简单的函数，可以使用 lambda表示法。  \n",
    "使用lambda的情况下，上述代码可以如下实现。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda w: net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dW=numerical_gradient(f,net.W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求出神经网络的梯度后，接下来只需根据梯度法，更新权重参数即可。  \n",
    "在下一节中，我们会以2层神经网络为例，实现整个学习过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了对应形状为多维数组的权重参数 W，这里使用的 numerical_gradient()和之前的实现稍有不同。  \n",
    "不过，改动只是为了对应多维数组，所以改动并不大。  \n",
    "这里省略了对代码的说明，想知道细节的读者请参考源代码（ common/gradient.py）。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5、学习算法的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络的学习步骤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 前提"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络存在合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为“学习”。  \n",
    "神经网络的学习分成下面4个步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 步骤1（ mini-batch）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从训练数据中随机选出一部分数据，这部分数据称为mini-batch。  \n",
    "我们的目标是减小mini-batch的损失函数的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 步骤2（计算梯度）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了减小mini-batch的损失函数的值，需要求出各个权重参数的梯度。  \n",
    "梯度表示损失函数的值减小最多的方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 步骤3（更新参数）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将权重参数沿梯度方向进行微小更新。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 步骤4（重复）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重复步骤1、步骤2、步骤3。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络的学习按照上面4个步骤进行。  \n",
    "这个方法通过梯度下降法更新参数，不过因为这里使用的数据是随机选择的mini batch数据，所以又称为**随机梯度下降法**（stochastic gradient descent）。  \n",
    "“随机”指的是“随机选择的”的意思，因此，随机梯度下降法是“对随机选择的数据进行的梯度下降法”。  \n",
    "深度学习的很多框架中，随机梯度下降法一般由一个名为SGD的函数来实现。  \n",
    "SGD来源于随机梯度下降法的英文名称的首字母。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面，我们来实现手写数字识别的神经网络。  \n",
    "这里以2层神经网络（隐藏层为1层的网络）为对象，使用MNIST数据集进行学习。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1、2层神经网络的类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们将这个2层神经网络实现为一个名为TwoLayerNet的类，实现\n",
    "过程如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size,\n",
    "        weight_init_std=0.01):\n",
    "        # 初始化权重\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "        np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "        np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        return y\n",
    "\n",
    "    # x:输入数据, t:监督数据\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    # x:输入数据, t:监督数据\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先把这个类中用到\n",
    "的变量和方法整理一下。表4-1中只罗列了重要的变量，表4-2中则罗列了所有的方法："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>表4-1　TwolayerNet类中使用的变量</center>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|变量 | 说明|\n",
    "|- | -|\n",
    "|params | 保存神经网络的参数的字典型变量（实例变量）。params['W1']是第1层的权重，params['b1']是第1层的偏置。params['W2']是第2层的权重， params['b2']是第2层的偏置。|\n",
    "|grads | 保存梯度的字典型变量（numerical_gradient()方法的返回值）。grads['W1']是第1层权重的梯度， grads['b1']是第1层偏置的梯度。grads['W2']是第2层权重的梯度， grads['b2']是第2层偏置的梯度|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>表4-2　TwoLayerNet类的方法</center>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|方法|说明|\n",
    "|-|-|\n",
    "|\\_\\_init__(self, input_size,hidden_size, output_size)|进行初始化。参数从头开始依次表示输入层的神经元数、隐藏层的神经元数、输出层的神经元数|\n",
    "|predict(self, x)|进行识别（推理）。参数x是图像数据|\n",
    "|loss(self, x, t)|计算损失函数的值。参数 x是图像数据， t是正确解标签（后面3个方法的参数也一样）|\n",
    "|accuracy(self, x, t)|计算识别精度|\n",
    "|numerical_gradient(self, x, t)|计算权重参数的梯度|\n",
    "|gradient(self, x, t)|计算权重参数的梯度。numerical_gradient()的高速版，将在下一章实现|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TwoLayerNet类有params和grads两个字典型实例变量。   \n",
    "params变量中保存了权重参数，比如params['W1']以NumPy数组的形式保存了第1层的权重参数。  \n",
    "此外，第1层的偏置可以通过param['b1']进行访问。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 100)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "net.params['W1'].shape # (784, 100)\n",
    "net.params['b1'].shape # (100,)\n",
    "net.params['W2'].shape # (100, 10)\n",
    "net.params['b2'].shape # (10,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上所示， params变量中保存了该神经网络所需的全部参数。  \n",
    "并且，params变量中保存的权重参数会用在推理处理（前向处理）中。顺便说一下，推理处理的实现如下所示。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(100, 784) # 伪输入数据（100笔）\n",
    "y = net.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外，与params变量对应， grads变量中保存了各个参数的梯度。  \n",
    "如下所示，使用 numerical_gradient()方法计算梯度后，梯度的信息将保存在 grads变\n",
    "量中。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 100)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.rand(100, 784) # 伪输入数据（100笔）\n",
    "t = np.random.rand(100, 10) # 伪正确解标签（100笔）\n",
    "grads = net.numerical_gradient(x, t) # 计算梯度\n",
    "grads['W1'].shape # (784, 100)\n",
    "grads['b1'].shape # (100,)\n",
    "grads['W2'].shape # (100, 10)\n",
    "grads['b2'].shape # (10,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着，我们来看一下 TwoLayerNet的方法的实现。  \n",
    "首先是 __init__(self,input_size, hidden_size, output_size)方法，它是类的初始化方法（所谓初始化方法，就是生成TwoLayerNet实例时被调用的方法）。  \n",
    "从第1个参数开始，依次表示输入层的神经元数、隐藏层的神经元数、输出层的神经元数。  \n",
    "另外，因为进行手写数字识别时，输入图像的大小是784（28 × 28），输出为10个类别，所以指定参数input_size=784、 output_size=10，将隐藏层的个数hidden_size设置为一个合适的值即可。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外，这个初始化方法会对权重参数进行初始化。  \n",
    "如何设置权重参数的初始值这个问题是关系到神经网络能否成功学习的重要问题。  \n",
    "后面我们会详细讨论权重参数的初始化，这里只需要知道，权重使用符合高斯分布的随机数进行初始化，偏置使用 0进行初始化。   \n",
    "predict(self, x)和accuracy(self, x, t)的实现和上一章的神经网络的推理处理基本一样。  \n",
    "如果仍有不明白的地方，请再回顾一下上一章的内容。另外， loss(self, x, t)是计算损失函数值的方法。这个方法会基于predict()的结果和正确解标签，\n",
    "计算交叉熵误差。\n",
    "剩下的 numerical_gradient(self, x, t)方法会计算各个参数的梯度。  \n",
    "根据数值微分，计算各个参数相对于损失函数的梯度。  \n",
    "另外， gradient(self, x, t)是下一章要实现的方法，该方法使用误差反向传播法高效地计算梯度。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numerical_gradient(self, x, t)基于数值微分计算参数的梯度。  \n",
    "下一章，我们会介绍一个高速计算梯度的方法，称为误差反向传播法。  \n",
    "用误差反向传播法求到的梯度和数值微分的结果基本一致，但可以高速地进行处理。  \n",
    "使用误差反向传播法计算梯度的 gradient(self,x, t)方法会在下一章实现，不过考虑到神经网络的学习比较花时间，想节约学习时间的读者可以替换掉这里的 numerical_gradient(self,x, t)，抢先使用gradient(self, x, t)！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2、mini-batch的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络的学习的实现使用的是前面介绍过的mini-batch学习。  \n",
    "所谓mini-batch学习，就是从训练数据中随机选择一部分数据（称为mini-batch），再以这些mini-batch为对象，使用梯度法更新参数的过程。  \n",
    "下面，我们就以TwoLayerNet类为对象，使用MNIST数据集进行学习。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "# from two_layer_net import TwoLayerNet\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label = True)\n",
    "\n",
    "train_loss_list = []\n",
    "# 超参数\n",
    "iters_num = 10\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 获取mini-batch\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 计算梯度\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    # grad = network.gradient(x_batch, t_batch) # 高速版!\n",
    "    \n",
    "    # 更新参数\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    # 记录学习过程\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里， mini-batch的大小为100，需要每次从60000个训练数据中随机取出100个数据（图像数据和正确解标签数据）。  \n",
    "然后，对这个包含100笔数据的mini-batch求梯度，使用随机梯度下降法（SGD）更新参数。  \n",
    "这里，梯度法的更新次数（循环的次数）为10000。  \n",
    "每更新一次，都对训练数据计算损失函数的值，并把该值添加到数组中。  \n",
    "用图像来表示这个损失函数的值的推移，如图4-11所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/4_11.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "观察图4-11，可以发现随着学习的进行，损失函数的值在不断减小。  \n",
    "这是学习正常进行的信号，表示神经网络的权重参数在逐渐拟合数据。  \n",
    "也就是说，神经网络的确在学习！  \n",
    "通过反复地向它浇灌（输入）数据，神经网络正在逐渐向最优参数靠近。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.3、基于测试数据的评价"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据图4-11呈现的结果，我们确认了通过反复学习可以使损失函数的值逐渐减小这一事实。  \n",
    "不过这个损失函数的值，严格地讲是“对训练数据的某个mini-batch的损失函数”的值。  \n",
    "训练数据的损失函数值减小，虽说是神经网络的学习正常进行的一个信号，但光看这个结果还不能说明该神经网络在其他数据集上也一定能有同等程度的表现。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络的学习中，必须确认是否能够正确识别训练数据以外的其他数据，即确认是否会发生过拟合。  \n",
    "过拟合是指，虽然训练数据中的数字图像能被正确辨别，  \n",
    "但是不在训练数据中的数字图像却无法被识别的现象。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络学习的最初目标是掌握泛化能力，因此，要评价神经网络的泛化能力，就必须使用不包含在训练数据中的数据。  \n",
    "下面的代码在进行学习的过程中，会定期地对训练数据和测试数据记录识别精度。  \n",
    "这里，每经过一个epoch，我们都会记录下训练数据和测试数据的识别精度。   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**epoch**是一个单位。  \n",
    "一个epoch表示学习中所有训练数据均被使用过一次时的更新次数。  \n",
    "比如，对于 10000笔训练数据，用大小为 100笔数据的mini-batch进行学习时，重复随机梯度下降法100次，所有的训练数据就都被“看过”了。此时，100次就是一个epoch。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了正确进行评价，我们来稍稍修改一下前面的代码。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.097, 0.0979\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "# from two_layer_net import TwoLayerNet\n",
    "\n",
    "# (x_train, t_train), (x_test, t_test) = \\ load_mnist(normalize=True, one_hot_label = True)\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label = True)\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "# 平均每个epoch的重复次数\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "# 超参数\n",
    "iters_num = 600\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 获取mini-batch\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 计算梯度\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    # grad = network.gradient(x_batch, t_batch) # 高速版!\n",
    "    \n",
    "    # 更新参数\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    # 计算每个epoch的识别精度\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的例子中，每经过一个epoch，就对所有的训练数据和测试数据计算识别精度，并记录结果。  \n",
    "之所以要计算每一个epoch的识别精度，是因为如果在for语句的循环中一直计算识别精度，会花费太多时间。  并且，也没有必要那么频繁地记录识别精度（只要从大方向上大致把握识别精度的推\n",
    "移就可以了）。  \n",
    "因此，我们才会每经过一个epoch就记录一次训练数据的识别精度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把从上面的代码中得到的结果用图表示的话，如图4-12所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/4_12.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图4-12中，实线表示训练数据的识别精度，虚线表示测试数据的识别精度。  \n",
    "如图所示，随着epoch的前进（学习的进行），我们发现使用训练数据和测试数据评价的识别精度都提高了，并且，这两个识别精度基本上没有差异（两条线基本重叠在一起）。  \n",
    "因此，可以说这次的学习中没有发生过拟合的现象。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 小结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1、机器学习中使用的数据集分为训练数据和测试数据。\n",
    "2、神经网络用训练数据进行学习，并用测试数据评价学习到的模型的\n",
    "泛化能力。\n",
    "3、神经网络的学习以损失函数为指标，更新权重参数，以使损失函数\n",
    "的值减小。\n",
    "4、利用某个给定的微小值的差分求导数的过程，称为数值微分。\n",
    "5、利用数值微分，可以计算权重参数的梯度。\n",
    "6、数值微分虽然费时间，但是实现起来很简单。下一章中要实现的稍微复杂一些的误差反向传播法可以高速地计算梯度。\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
